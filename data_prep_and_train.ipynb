{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.datasets import CSVClassificationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/TrainNER.csv\", sep=\";\", encoding = \"ISO-8859-1\")\n",
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_sentence_numbers(df):\n",
    "    sentences = df[\"Sentence #\"].values\n",
    "    current = sentences[0]\n",
    "    new_sentence_nums = []\n",
    "    for s in sentences:\n",
    "        if s != '':\n",
    "            current = s\n",
    "        new_sentence_nums.append(int(current.replace(\"Sentence: \", \"\")))\n",
    "    df[\"Sentence #\"] = new_sentence_nums\n",
    "        \n",
    "fill_sentence_numbers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>President</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Karzai</td>\n",
       "      <td>NNP</td>\n",
       "      <td>I-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>thanked</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>his</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>allies</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196640</th>\n",
       "      <td>9000</td>\n",
       "      <td>prices</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196641</th>\n",
       "      <td>9000</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196642</th>\n",
       "      <td>9000</td>\n",
       "      <td>foreign</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196643</th>\n",
       "      <td>9000</td>\n",
       "      <td>investment</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196644</th>\n",
       "      <td>9000</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196645 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence #        Word   POS    Tag\n",
       "0                1   President   NNP  B-per\n",
       "1                1      Karzai   NNP  I-per\n",
       "2                1     thanked   VBD      O\n",
       "3                1         his  PRP$      O\n",
       "4                1      allies   NNS      O\n",
       "...            ...         ...   ...    ...\n",
       "196640        9000      prices   NNS      O\n",
       "196641        9000         and    CC      O\n",
       "196642        9000     foreign    JJ      O\n",
       "196643        9000  investment    NN      O\n",
       "196644        9000           .     .      O\n",
       "\n",
       "[196645 rows x 4 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"Word\"] != \"\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df[\"Sentence #\"] <= 6000]\n",
    "dev = df[(df[\"Sentence #\"] > 6000) & (df[\"Sentence #\"] >= 7500)]\n",
    "test = df[df[\"Sentence #\"] > 7500]\n",
    "\n",
    "def write_in_proper_format(df, filename):\n",
    "    prev_sentence_num = df.iloc[0][\"Sentence #\"]\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        current_sentence_num = row[\"Sentence #\"]\n",
    "        \n",
    "        with open(filename, \"a\") as file:\n",
    "            if current_sentence_num != prev_sentence_num:\n",
    "                file.write(\"\\n\")\n",
    "            file.write(row[\"Word\"])\n",
    "            file.write(\" \")\n",
    "            file.write(row[\"POS\"])\n",
    "            file.write(\" \")\n",
    "            file.write(row[\"Tag\"])\n",
    "            file.write(\"\\n\")\n",
    "        prev_sentence_num = current_sentence_num\n",
    "\n",
    "output_csv_train = \"./data/train.txt\"\n",
    "output_csv_dev = \"./data/dev.txt\"\n",
    "output_csv_test = \"./data/test.txt\"\n",
    "write_in_proper_format(train, output_csv_train)\n",
    "write_in_proper_format(dev, output_csv_dev)\n",
    "write_in_proper_format(test, output_csv_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-30 20:10:40,980 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to C:\\Users\\Marci\\AppData\\Local\\Temp\\tmpt2_hym2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000128/160000128 [03:44<00:00, 712776.32B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-30 20:14:25,726 copying C:\\Users\\Marci\\AppData\\Local\\Temp\\tmpt2_hym2s to cache at C:\\Users\\Marci\\.flair\\embeddings\\glove.gensim.vectors.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-30 20:14:26,432 removing temp file C:\\Users\\Marci\\AppData\\Local\\Temp\\tmpt2_hym2s\n",
      "2020-09-30 20:14:26,815 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim not found in cache, downloading to C:\\Users\\Marci\\AppData\\Local\\Temp\\tmpaysghglf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21494764/21494764 [00:30<00:00, 706419.64B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-30 20:14:57,463 copying C:\\Users\\Marci\\AppData\\Local\\Temp\\tmpaysghglf to cache at C:\\Users\\Marci\\.flair\\embeddings\\glove.gensim\n",
      "2020-09-30 20:14:57,495 removing temp file C:\\Users\\Marci\\AppData\\Local\\Temp\\tmpaysghglf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-30 20:14:58,698 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/news-backward-0.4.1.pt not found in cache, downloading to C:\\Users\\Marci\\AppData\\Local\\Temp\\tmpg9go46w4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73034575/73034575 [01:43<00:00, 703974.34B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-30 20:16:42,584 copying C:\\Users\\Marci\\AppData\\Local\\Temp\\tmpg9go46w4 to cache at C:\\Users\\Marci\\.flair\\embeddings\\news-backward-0.4.1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-30 20:16:42,692 removing temp file C:\\Users\\Marci\\AppData\\Local\\Temp\\tmpg9go46w4\n",
      "2020-09-30 20:16:43,670 ----------------------------------------------------------------------------------------------------\n",
      "2020-09-30 20:16:43,672 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings('glove')\n",
      "    (list_embedding_1): PooledFlairEmbeddings(\n",
      "      (context_embeddings): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_2): PooledFlairEmbeddings(\n",
      "      (context_embeddings): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.05, inplace=False)\n",
      "          (encoder): Embedding(300, 100)\n",
      "          (rnn): LSTM(100, 2048)\n",
      "          (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=8292, out_features=8292, bias=True)\n",
      "  (rnn): LSTM(8292, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=18, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-09-30 20:16:43,673 ----------------------------------------------------------------------------------------------------\n",
      "2020-09-30 20:16:43,673 Corpus: \"Corpus: 6000 train + 1501 dev + 1500 test sentences\"\n",
      "2020-09-30 20:16:43,674 ----------------------------------------------------------------------------------------------------\n",
      "2020-09-30 20:16:43,675 Parameters:\n",
      "2020-09-30 20:16:43,677  - learning_rate: \"0.1\"\n",
      "2020-09-30 20:16:43,678  - mini_batch_size: \"32\"\n",
      "2020-09-30 20:16:43,678  - patience: \"3\"\n",
      "2020-09-30 20:16:43,679  - anneal_factor: \"0.5\"\n",
      "2020-09-30 20:16:43,681  - max_epochs: \"150\"\n",
      "2020-09-30 20:16:43,683  - shuffle: \"True\"\n",
      "2020-09-30 20:16:43,683  - train_with_dev: \"True\"\n",
      "2020-09-30 20:16:43,685  - batch_growth_annealing: \"False\"\n",
      "2020-09-30 20:16:43,685 ----------------------------------------------------------------------------------------------------\n",
      "2020-09-30 20:16:43,686 Model training base path: \"resources\\taggers\\example-ner\"\n",
      "2020-09-30 20:16:43,686 ----------------------------------------------------------------------------------------------------\n",
      "2020-09-30 20:16:43,687 Device: cpu\n",
      "2020-09-30 20:16:43,687 ----------------------------------------------------------------------------------------------------\n",
      "2020-09-30 20:16:43,689 Embeddings storage mode: cpu\n",
      "2020-09-30 20:16:43,696 ----------------------------------------------------------------------------------------------------\n",
      "train mode resetting embeddings\n",
      "train mode resetting embeddings\n",
      "2020-09-30 20:22:37,014 epoch 1 - iter 23/235 - loss 20.32091775 - samples/sec: 2.10 - lr: 0.100000\n",
      "2020-09-30 20:28:52,593 epoch 1 - iter 46/235 - loss 14.57955018 - samples/sec: 1.97 - lr: 0.100000\n",
      "2020-09-30 20:34:24,258 epoch 1 - iter 69/235 - loss 11.79078412 - samples/sec: 2.27 - lr: 0.100000\n",
      "2020-09-30 20:40:28,375 epoch 1 - iter 92/235 - loss 10.09885119 - samples/sec: 2.09 - lr: 0.100000\n",
      "2020-09-30 20:41:10,934 ----------------------------------------------------------------------------------------------------\n",
      "2020-09-30 20:41:10,939 Exiting from training early.\n",
      "2020-09-30 20:41:10,942 Saving model ...\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, PooledFlairEmbeddings\n",
    "from typing import List\n",
    "\n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "\n",
    "# initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "\n",
    "    # GloVe embeddings\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # contextual string embeddings, forward\n",
    "    PooledFlairEmbeddings('news-forward', pooling='min'),\n",
    "\n",
    "    # contextual string embeddings, backward\n",
    "    PooledFlairEmbeddings('news-backward', pooling='min'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type)\n",
    "\n",
    "# initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "trainer.train('resources/taggers/szwb-ner',\n",
    "              train_with_dev=True,  \n",
    "              max_epochs=150,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
